{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMlYgEEiWz+mU5V449b8jp7"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"ffbAi_I_dhhx"},"outputs":[],"source":["\n","# Instalar PySpark\n","!pip -q install pyspark==3.5.1\n","\n","# Montar o Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Caminho base do projeto (pasta Eixo_05)\n","base_path = \"/content/drive/MyDrive/Eixo_05/dados/\""]},{"cell_type":"code","source":["# Iniciar sessão Spark\n","from pyspark.sql import SparkSession\n","\n","spark = (\n","    SparkSession.builder\n","    .appName(\"imdb-processamento\")\n","    .getOrCreate()\n",")\n","spark\n"],"metadata":{"id":"zvpyiFy5dnbI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# Imports necessários\n","from pyspark.sql.functions import col, lower, regexp_replace\n","from pyspark.ml.feature import (\n","    StringIndexer, RegexTokenizer, StopWordsRemover,\n","    HashingTF, IDF, Word2Vec, MinMaxScaler\n",")\n","\n","# Função auxiliar para valores nulos\n","def calcula_valores_nulos(df):\n","    null_columns_counts = []\n","    numRows = df.count()\n","    for k in df.columns:\n","        nullRows = df.where(col(k).isNull()).count()\n","        if nullRows > 0:\n","            null_columns_counts.append((k, nullRows, (nullRows / numRows) * 100))\n","    return null_columns_counts"],"metadata":{"id":"2ANo8jB_dpPr"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Função principal de limpeza e transformação\n","def limpar_transformar(spark, base_path):\n","    # 1) Ler o CSV salvo no Drive\n","    #reviews = spark.read.csv(base_path + \"dataset.csv\", header=True, escape=\"\\\"\").limit(5000)\n","    reviews = spark.read.csv(base_path + \"dataset.csv\", header=True, escape=\"\\\"\")\n","\n","    # 2) Tratar valores nulos\n","    nulos = calcula_valores_nulos(reviews)\n","    if nulos:\n","        reviews = reviews.dropna()\n","\n","    # 3) Indexar rótulos (sentiment -> label)\n","    indexer = StringIndexer(inputCol=\"sentiment\", outputCol=\"label\")\n","    df = indexer.fit(reviews).transform(reviews)\n","\n","    # 4) Limpeza de texto\n","    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '<.*/>', ''))\n","    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), '[^A-Za-z ]+', ''))\n","    df = df.withColumn(\"review\", regexp_replace(col(\"review\"), ' +', ' '))\n","    df = df.withColumn(\"review\", lower(col(\"review\")))\n","\n","    # 5) Tokenização e remoção de stopwords\n","    df = RegexTokenizer(inputCol=\"review\", outputCol=\"words\", pattern=\"\\W\").transform(df)\n","    feature_data = StopWordsRemover(inputCol=\"words\", outputCol=\"filtered\").transform(df)\n","\n","    # 6) HashingTF\n","    htf = HashingTF(inputCol=\"filtered\", outputCol=\"rawfeatures\", numFeatures=250)\n","    HTFfeaturizedData = htf.transform(feature_data)\n","\n","    # 7) TF-IDF\n","    idf = IDF(inputCol=\"rawfeatures\", outputCol=\"features\")\n","    idfModel = idf.fit(HTFfeaturizedData)\n","    TFIDFfeaturizedData = idfModel.transform(HTFfeaturizedData)\n","\n","    # 8) Word2Vec + MinMaxScaler\n","    w2v = Word2Vec(vectorSize=250, minCount=5, inputCol=\"filtered\", outputCol=\"features\")\n","    W2VfeaturizedData = w2v.fit(feature_data).transform(feature_data)\n","\n","    scaler = MinMaxScaler(inputCol=\"features\", outputCol=\"scaledFeatures\")\n","    scaled = scaler.fit(W2VfeaturizedData).transform(W2VfeaturizedData)\n","    W2VfeaturizedData = scaled.select(\"sentiment\", \"review\", \"label\", \"scaledFeatures\") \\\n","                              .withColumnRenamed(\"scaledFeatures\", \"features\")\n","\n","    # 9) Salvar resultados no Drive\n","    (HTFfeaturizedData\n","     .select(\"sentiment\", \"review\", \"label\", \"rawfeatures\")\n","     .withColumnRenamed(\"rawfeatures\", \"features\")\n","     .write.mode(\"overwrite\").parquet(base_path + \"HTFfeaturizedData\"))\n","\n","    TFIDFfeaturizedData.select(\"sentiment\", \"review\", \"label\", \"features\") \\\n","        .write.mode(\"overwrite\").parquet(base_path + \"TFIDFfeaturizedData\")\n","\n","    W2VfeaturizedData.select(\"sentiment\", \"review\", \"label\", \"features\") \\\n","        .write.mode(\"overwrite\").parquet(base_path + \"W2VfeaturizedData\")\n","\n","    return (\n","        HTFfeaturizedData.select(\"sentiment\", \"review\", \"label\", \"rawfeatures\")\n","                         .withColumnRenamed(\"rawfeatures\", \"features\"),\n","        TFIDFfeaturizedData.select(\"sentiment\", \"review\", \"label\", \"features\"),\n","        W2VfeaturizedData.select(\"sentiment\", \"review\", \"label\", \"features\"),\n","    )\n"],"metadata":{"id":"lkaZFHIZdq7S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Executar processamento e salvar resultados no Drive\n","HTF, TFIDF, W2V = limpar_transformar(spark, base_path=base_path)\n","\n","print(\"Dados processados e salvos no Google Drive em:\")\n","print(f\"{base_path}HTFfeaturizedData\")\n","print(f\"{base_path}TFIDFfeaturizedData\")\n","print(f\"{base_path}W2VfeaturizedData\")\n","\n","print(\"Contagens:\", HTF.count(), TFIDF.count(), W2V.count())"],"metadata":{"id":"ag-8rQ1PduAH"},"execution_count":null,"outputs":[]}]}